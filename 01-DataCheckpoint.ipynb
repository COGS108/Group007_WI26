
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric\n",
    "\n",
    "Instructions: DELETE this cell before you submit via a `git push` to your repo before deadline. This cell is for your reference only and is not needed in your report. \n",
    "\n",
    "Scoring: Out of 10 points\n",
    "\n",
    "- Each Developing  => -2 pts\n",
    "- Each Unsatisfactory/Missing => -4 pts\n",
    "  - until the score is \n",
    "\n",
    "If students address the detailed feedback in a future checkpoint they will earn these points back\n",
    "\n",
    "\n",
    "|                  | Unsatisfactory                                                                                                                                                                                                    | Developing                                                                                                                                                                                              | Proficient                                     | Excellent                                                                                                                              |\n",
    "|------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Data relevance   | Did not have data relevant to their question. Or the datasets don't work together because there is no way to line them up against each other. If there are multiple datasets, most of them have this trouble | Data was only tangentially relevant to the question or a bad proxy for the question. If there are multiple datasets, some of them may be irrelevant or can't be easily combined.                       | All data sources are relevant to the question. | Multiple data sources for each aspect of the project. It's clear how the data supports the needs of the project.                         |\n",
    "| Data description | Dataset or its cleaning procedures are not described. If there are multiple datasets, most have this trouble                                                                                              | Data was not fully described. If there are multiple datasets, some of them are not fully described                                                                                                      | Data was fully described                       | The details of the data descriptions and perhaps some very basic EDA also make it clear how the data supports the needs of the project. |\n",
    "| Data wrangling   | Did not obtain data. They did not clean/tidy the data they obtained.  If there are multiple datasets, most have this trouble                                                                                 | Data was partially cleaned or tidied. Perhaps you struggled to verify that the data was clean because they did not present it well. If there are multiple datasets, some have this trouble | The data is cleaned and tidied.                | The data is spotless and they used tools to visualize the data cleanliness and you were convinced at first glance                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
"## Authors\n",
"\n",
"- Nicholas Campos: Conceptualization, Methodology, Writing – original draft\n",
"\n",
"- Lui Gazem: Analysis, Writing – original draft\n",
"\n",
"- Cadence Eastep: Background research, Writing – original draft, Writing – review & editing\n",
"\n",
"- Ahgean Davis: Data curation, Software, Visualization\n",
"\n",
"- Chuck Davies: Project administration, Ethics review, Writing – review & editing\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Primary research question:**  \n",
    "How do daily COVID-19 case counts vary by day of the week at the county level, and how are these variations associated with mobility patterns (workplace, residential, retail & recreation, and transit mobility)?\n",
    "\n",
    "**Secondary (backup) research question:**  \n",
    "To what extent are changes in workplace and residential mobility associated with subsequent changes in daily COVID-19 case counts over time at the county level?\n",
    "\n",
    "**Variables and controls:**\n",
    "- **Dependent variable (DV):** daily new COVID-19 case counts (county-day level)\n",
    "- **Independent variables (IVs):** workplace mobility, residential mobility, retail & recreation mobility, transit mobility, day of week\n",
    "- **Controls:** county fixed effects, time (date or week index), population (if available), lagged case counts\n",
    "\n",
    "**Note:** We discussed our primary research question with the TA, who indicated it was appropriate but recommended including a backup question in case broader scope is needed after exploratory data analysis. Both questions use the same county-level COVID-19 and mobility dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [ "Mobility rates and COVID-19 case counts exhibited a substantial correlation throughout the pandemic. In response to the rapid spread of the virus, public health authorities urged individuals to remain at home whenever possible, recognizing that increased movement and social interaction heightened the risk of transmission. From the onset of this global crisis, policymakers closely monitored the relationship between population mobility and infection rates in an effort to mitigate uncontrolled surges in cases. A study conducted by the Centers for Disease Control and Prevention (CDC) examined anonymized cell phone GPS data to assess patterns of movement across counties in the United States. The researchers observed that reductions in trips outside the home were associated with subsequent declines in reported COVID-19 cases. Specifically, changes in mobility were analyzed in relation to new case counts approximately 11 days later, reflecting the typical incubation period and reporting delays. The study also compared urban and rural areas to determine whether mobility patterns affected transmission differently across settings. This distinction was critical, as urban centers tend to be more densely populated, exhibit distinct transportation behaviors, and may experience differing timelines of viral introduction and spread compared to rural communities.\n",
    "\n",
    "For our project, we aim to more closely examine variation in COVID-19 case counts across days of the week, with particular attention to identifying when case spikes are most pronounced. Consistent with our hypothesis, we anticipate that reported cases will be higher during weekdays compared to weekends. One possible explanation is that essential workers continued in-person employment throughout much of the pandemic, increasing opportunities for exposure during the workweek. Workplace environments may facilitate transmission due to repeated close interactions with coworkers and contact with shared surfaces (e.g., counters, equipment, communal appliances). These routine occupational exposures could contribute to elevated transmission risk during weekdays, which may subsequently be reflected in higher reported case counts. This hypothesis is supported by prior empirical work conducted by Hannah Hoffman and The COVID Tracking Project, which examined temporal reporting patterns in state-level COVID-19 case data. Their analysis demonstrated a consistent decline in reported case counts over weekends. Importantly, this pattern was attributed not only to behavioral changes but also to structural factors, including reduced testing, reporting delays, and public health regulations that limited activity during weekends in many states at the time.\n",
    "\n",
    "In a study conducted by Bulut Boru and M. Emre Gursoy, the authors developed a computational model designed to identify and learn patterns between human mobility and subsequent COVID-19 case counts. Their approach dynamically selects the optimal time frame and predictive method to enhance accuracy. When evaluated across 13 countries, the model demonstrated a high degree of precision in forecasting daily COVID-19 cases. The study further revealed a strong correlation between the movement patterns of individuals and the spread of the virus within their communities. Findings of this nature underscore the substantial impact that human mobility can have on disease transmission, providing empirical justification for the strict travel restrictions and mobility-limiting policies implemented by governments worldwide during the pandemic.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hypothesize that daily COVID-19 case counts (daily_cases) will vary systematically across days of the week, with lower reported cases on weekends (is_weekend = 1) and higher reported cases during weekdays. Additionally, we predict that increases in workplace mobility (workplaces) will be positively associated with daily COVID-19 case counts, while increases in residential mobility (residential) will be negatively associated with daily case counts at the county level over time.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data overview\n",
    "\n",
    "Instructions: REPLACE the contents of this cell with descriptions of your actual datasets.\n",
    "\n",
    "For each dataset include the following information\n",
    "- Dataset #1\n",
    "  - Dataset Name:\n",
    "  - Link to the dataset:\n",
    "  - Number of observations:\n",
    "  - Number of variables:\n",
    "  - Description of the variables most relevant to this project\n",
    "  - Descriptions of any shortcomings this dataset has with repsect to the project\n",
    "- Dataset #2 (if you have more than one!)\n",
    "  - same as above\n",
    "- etc\n",
    "\n",
    "Each dataset deserves either a set of bullet points as above or a few sentences if you prefer that method.\n",
    "\n",
    "If you plan to use multiple datasets, add a few sentences about how you plan to combine these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code every time when you're actively developing modules in .py files.  It's not needed if you aren't making modules\n",
    "#\n",
    "## this code is necessary for making sure that any modules we load are updated here \n",
    "## when their source code .py files are modified\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup code -- this only needs to be run once after cloning the repo!\n",
    "# this code downloads the data from its source to the `data/00-raw/` directory\n",
    "# if the data hasn't updated you don't need to do this again!\n",
    "\n",
    "# if you don't already have these packages (you should!) uncomment this line\n",
    "# %pip install requests tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('./modules') # this tells python where to look for modules to import\n",
    "\n",
    "import get_data # this is where we get the function we need to download data\n",
    "\n",
    "# replace the urls and filenames in this list with your actual datafiles\n",
    "# yes you can use Google drive share links or whatever\n",
    "# format is a list of dictionaries; \n",
    "# each dict has keys of \n",
    "#   'url' where the resource is located\n",
    "#   'filename' for the local filename where it will be stored \n",
    "datafiles = [\n",
    "    { 'url': 'https://raw.githubusercontent.com/fivethirtyeight/data/refs/heads/master/airline-safety/airline-safety.csv', 'filename':'airline-safety.csv'},\n",
    "    { 'url': 'https://raw.githubusercontent.com/fivethirtyeight/data/refs/heads/master/bad-drivers/bad-drivers.csv', 'filename':'bad-drivers.csv'}\n",
    "]\n",
    "\n",
    "get_data.get_raw(datafiles,destination_directory='data/00-raw/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #1: County-Level Daily COVID-19 Case and Mobility Data (IntegratedData3)\n",
    "\n",
    "This dataset contains daily reported COVID-19 case counts merged with mobility metrics at the county-day level across the United States. Each row represents one county on one specific date. The dataset spans from early 2021 onward and covers every county in the US.\n",
    "\n",
    "The variables most relevant to this project are as follows. The `date` column records the calendar date in YYYY-MM-DD format. The `county`, `state`, and `fips` columns identify the geographic location - `fips` is a unique numeric code assigned to every US county and was used as the key to merge the COVID and mobility data together. The `cases` column is the cumulative total confirmed COVID-19 cases in that county up to that date (count). The `daily_cases` column is the number of new confirmed COVID-19 cases reported on that specific day (count) and is our **primary dependent variable**. Values are typically positive integers but can occasionally be zero or negative - negative values occur when counties issue retroactive corrections to previously reported counts. The `day_of_week` column encodes the day numerically (0 = Monday through 6 = Sunday) and `is_weekend` is a binary flag (1 = Saturday or Sunday, 0 = weekday). These are our key independent variables for examining weekday vs. weekend patterns. The `is_holiday` column flags US federal holidays. The mobility columns `workplaces` and `residential` are our key mobility predictors - both represent percent change from a pre-pandemic baseline (the median value for the same day of the week during January 3 to February 6, 2020). A value of -20 in `workplaces` means 20% fewer people at workplaces than baseline. A positive value in `residential` means people spent more time at home than baseline. The `retail_recreation`, `grocery_pharmacy`, `parks`, and `transit` columns follow the same percent-change-from-baseline format.\n",
    "\n",
    "This dataset has several important limitations. First, COVID-19 case counts depend heavily on testing access and reporting practices, which varied significantly across counties and over time. Reported `daily_cases` reflects both true infections and the availability of tests, meaning counties with less testing infrastructure will appear to have fewer cases even if true infection rates were similar. Second, there is a well-documented weekend reporting effect where fewer tests are processed and reported on Saturdays and Sundays, causing `daily_cases` to artificially dip on weekends - this directly interacts with our primary research question and we account for it in our analysis. Third, negative `daily_cases` values are data artifacts from retroactive corrections and must be handled during cleaning. Fourth, the `transit` and `parks` mobility columns have substantial missingness, particularly in smaller and rural counties where Google had insufficient location data. Fifth, mobility data is derived from smartphone GPS signals and underrepresents populations without smartphones or location services enabled.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #2 \n",
    "\n",
    "See instructions above for Dataset #1.  Feel free to keep adding as many more datasets as you need.  Put each new dataset in its own section just like these. \n",
    "\n",
    "Lastly if you do have multiple datasets, add another section where you demonstrate how you will join, align, cross-reference or whatever to combine data from the different datasets\n",
    "\n",
    "Please note that you can always keep adding more datasets in the future if these datasets you turn in for the checkpoint aren't sufficient.  The goal here is demonstrate that you can obtain and wrangle data.  You are not tied down to only use what you turn in right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Load the raw integrated dataset from Google Drive\n",
    "url = \"https://drive.google.com/uc?export=download&id=1hQwLJdeMyXGf2Fp0xEBXQrcj8sZsRYca\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Show size of dataset\n",
    "print('Dataset shape:', df.shape)\n",
    "print('Columns:', list(df.columns))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and convert date column\n",
    "print(df.dtypes)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "print('Date range:', df['date'].min(), 'to', df['date'].max())\n",
    "print('Unique counties:', df['fips'].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_count = df.isnull().sum()\n",
    "missing_pct = (df.isnull().mean() * 100).round(2)\n",
    "missing_df = pd.DataFrame({'missing_count': missing_count, 'missing_pct (%)': missing_pct})\n",
    "print(missing_df)\n",
    "\n",
    "# Visualize missingness\n",
    "cols_with_missing = missing_pct[missing_pct > 0]\n",
    "if len(cols_with_missing) > 0:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    cols_with_missing.sort_values().plot(kind='barh', color='steelblue', edgecolor='black')\n",
    "    plt.xlabel('% Missing')\n",
    "    plt.title('Percentage of Missing Values by Column')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No missing values found.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if missingness is systematic (smaller counties missing more)\n",
    "df['transit_missing'] = df['transit'].isnull().astype(int)\n",
    "df['parks_missing'] = df['parks'].isnull().astype(int)\n",
    "print('Avg cases where transit PRESENT vs MISSING:')\n",
    "print(df.groupby('transit_missing')['cases'].mean())\n",
    "print('Avg cases where parks PRESENT vs MISSING:')\n",
    "print(df.groupby('parks_missing')['cases'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for negative daily_cases and duplicates\n",
    "neg_cases = df[df['daily_cases'] < 0]\n",
    "print('Rows with negative daily_cases:', len(neg_cases))\n",
    "print(neg_cases[['date', 'county', 'state', 'daily_cases']].head(10))\n",
    "print('daily_cases summary:')\n",
    "print(df['daily_cases'].describe())\n",
    "dupes = df.duplicated(subset=['date', 'fips']).sum()\n",
    "print('Duplicate county-date rows:', dupes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the dataset\n",
    "df_clean = df.copy()\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "print('After dropping duplicates:', df_clean.shape[0], 'rows')\n",
    "\n",
    "# Flag negative daily_cases - keep rows but mark them\n",
    "df_clean['negative_case_flag'] = (df_clean['daily_cases'] < 0).astype(int)\n",
    "print('Rows flagged as negative daily_cases:', df_clean['negative_case_flag'].sum())\n",
    "\n",
    "# Clip negatives to 0 for analysis\n",
    "df_clean['daily_cases_clean'] = df_clean['daily_cases'].clip(lower=0)\n",
    "\n",
    "# Drop temp columns\n",
    "df_clean = df_clean.drop(columns=['transit_missing', 'parks_missing'])\n",
    "print('Final clean dataset shape:', df_clean.shape)\n",
    "df_clean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify tidy - one row per county per date\n",
    "unique_pairs = df_clean.drop_duplicates(subset=['date', 'fips']).shape[0]\n",
    "total_rows = df_clean.shape[0]\n",
    "print('Unique county-date pairs:', unique_pairs)\n",
    "print('Total rows:', total_rows)\n",
    "if unique_pairs == total_rows:\n",
    "    print('Dataset is tidy - one row per county per date.')\n",
    "else:\n",
    "    print('Warning: duplicate county-date rows exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for key variables\n",
    "key_cols = ['daily_cases_clean', 'workplaces', 'residential', 'retail_recreation', 'transit', 'is_weekend']\n",
    "print('Summary statistics for key variables:')\n",
    "df_clean[key_cols].describe().round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned dataset\n",
    "os.makedirs('data/02-processed', exist_ok=True)\n",
    "df_clean.to_csv('data/02-processed/covid_mobility_clean.csv', index=False)\n",
    "print('Saved to data/02-processed/covid_mobility_clean.csv')\n",
    "print('Final shape:', df_clean.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning summary:** The raw dataset contained one row per US county per day. The `transit` and `parks` columns had the highest missingness, confirmed to be systematic - counties with missing mobility data tend to be smaller where Google had insufficient location data. We retained NaN values rather than imputing since filling them would misrepresent a true absence of data. Negative `daily_cases` values from retroactive corrections were flagged and clipped to zero in `daily_cases_clean`. No duplicate county-date rows were found. The cleaned file was saved to `data/02-processed/covid_mobility_clean.csv`.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
  "## Ethics \n",
  "\n",
  "Instructions: Keep the contents of this cell. For each item on the checklist\n",
  "-  put an X there if you've considered the item\n",
  "-  IF THE ITEM IS RELEVANT place a short paragraph after the checklist item discussing the issue.\n",
  "  \n",
  "Items on this checklist are meant to provoke discussion among good-faith actors who take their ethical responsibilities seriously. Your teams will document these discussions and decisions for posterity using this section.  You don't have to solve these problems, you just have to acknowledge any potential harm no matter how unlikely.\n",
  "\n",
  "Here is a [list of real world examples](https://deon.drivendata.org/examples/) for each item in the checklist that can refer to.\n",
  "\n",
  "[![Deon badge](https://img.shields.io/badge/ethics%20checklist-deon-brightgreen.svg?style=popout-square)](http://deon.drivendata.org/)\n",
  "\n",
  "### A. Data Collection\n",
  " - [X] **A.1 Informed consent**: If there are human subjects, have they given informed consent, where subjects affirmatively opt-in and have a clear understanding of the data uses to which they consent?\n",
  "\n",
  "Our analysis uses publicly available, aggregated county-level COVID-19 case counts and mobility metrics. We do not recruit participants, interact with individuals, or collect new individual-level data, so informed consent is not required for our work. However, we acknowledge that mobility metrics originate from smartphone location signals and that many users may not fully understand downstream secondary uses, even when data is anonymized and aggregated.\n",
  "\n",
  " - [X] **A.2 Collection bias**: Have we considered sources of bias that could be introduced during data collection and survey design and taken steps to mitigate those?\n",
  "\n",
  "Mobility data can underrepresent people without smartphones or stable location services (for example older adults, low income communities, unhoused populations, and some rural areas). COVID-19 case counts also reflect testing access and reporting practices that vary by county and over time. We will interpret findings as population-level associations and avoid claims that assume equal measurement quality across all counties or groups.\n",
  "\n",
  " - [X] **A.3 Limit PII exposure**: Have we considered ways to minimize exposure of personally identifiable information (PII) for example through anonymization or not collecting information that isn't relevant for analysis?\n",
  "\n",
  "Our dataset is already aggregated to county-day level and does not include direct identifiers (names, addresses, device IDs). We will not attempt any re-identification or linkage to external individual-level datasets. We will also avoid presenting results as county rankings intended to label places as good or bad.\n",
  "\n",
  " - [X] **A.4 Downstream bias mitigation**: Have we considered ways to enable testing downstream results for biased outcomes (e.g., collecting data on protected group status like race or gender)?\n",
  "\n",
  "Our results could be misused to blame specific regions or communities for COVID-19 spread or to justify punitive policy based on mobility patterns. We will present conclusions carefully, emphasizing that mobility is a proxy for many structural factors and that associations do not imply individual fault or causation. Where feasible, we will add contextual covariates (for example population, time controls, fixed effects) and include limitations that discourage stigmatizing interpretations.\n",
  "\n",
  "### B. Data Storage\n",
  " - [X] **B.1 Data security**: Do we have a plan to protect and secure data (e.g., encryption at rest and in transit, access controls on internal users and third parties, access logs, and up-to-date software)?\n",
  "\n",
  "The data are public and aggregated, so the risk level is low, but we will still follow basic security practices: keep data in the course GitHub repo, avoid sharing raw data outside the team, and ensure devices/accounts used to access the repo are protected. We will not store any private keys or credentials in the repository.\n",
  "\n",
  " - [X] **B.2 Right to be forgotten**: Do we have a mechanism through which an individual can request their personal information be removed?\n",
  "\n",
  "Because the dataset does not contain individual-level identifiers, individuals cannot be singled out or removed from our data. We rely on the data providers' governance and opt-out mechanisms (if any). Our project does not maintain user-level records.\n",
  "\n",
  " - [X] **B.3 Data retention plan**: Is there a schedule or plan to delete the data after it is no longer needed?\n",
  "\n",
  "We will keep the dataset only as long as needed for the course and reproducibility of the final report. After the course, we can remove large raw data files from the repo while retaining derived results and code that do not contain sensitive information.\n",
  "\n",
  "### C. Analysis\n",
  " - [X] **C.1 Missing perspectives**: Have we sought to address blindspots in the analysis through engagement with relevant stakeholders (e.g., checking assumptions and discussing implications with affected communities and subject matter experts)?\n",
  "\n",
  "Public health outcomes depend on factors not captured by mobility (healthcare access, workplace protections, housing density, local policy, and reporting practices). We will check our assumptions using public health context from reputable sources and discuss limitations so our results are not interpreted as purely behavioral explanations.\n",
  "\n",
  " - [X] **C.2 Dataset bias**: Have we examined the data for possible sources of bias and taken steps to mitigate or address these biases (e.g., stereotype perpetuation, confirmation bias, imbalanced classes, or omitted confounding variables)?\n",
  "\n",
  "We expect bias from uneven testing/reporting, time-varying policy changes, and demographic differences across counties. We will use appropriate controls (time effects, county fixed effects, lagged outcomes where relevant) and perform basic robustness checks (for example examining patterns across time periods) to reduce the chance of misleading conclusions.\n",
  "\n",
  " - [X] **C.3 Honest representation**: Are our visualizations, summary statistics, and reports designed to honestly represent the underlying data?\n",
  "\n",
  "We will avoid causal wording and report results as associations. Visualizations will clearly label units and transformations, and we will include uncertainty or variability where possible. We will not cherry-pick dates or counties to support a preferred conclusion.\n",
  "\n",
  " - [X] **C.4 Privacy in analysis**: Have we ensured that data with PII are not used or displayed unless necessary for the analysis?\n",
  "\n",
  "No PII is present in the dataset and none will be introduced. We will only analyze and display aggregated county-level metrics.\n",
  "\n",
  " - [X] **C.5 Auditability**: Is the process of generating the analysis well documented and reproducible if we discover issues in the future?\n",
  "\n",
  "All preprocessing, analysis steps, and figure generation will be documented in notebooks/scripts in the repository. This supports reproducibility and makes it easier to identify issues if errors or data problems are later discovered.\n",
  "\n",
  "### D. Modeling\n",
  " - [X] **D.1 Proxy discrimination**: Have we ensured that the model does not rely on variables or proxies for variables that are unfairly discriminatory?\n",
  "\n",
  "Mobility measures can correlate with socioeconomic status and occupation types. We will not use models for individual-level prediction, and we will interpret results as structural and measurement patterns rather than traits of any group.\n",
  "\n",
  " - [X] **D.2 Fairness across groups**: Have we tested model results for fairness with respect to different affected groups (e.g., tested for disparate error rates)?\n",
  "\n",
  "We are not building a high-stakes decision system, but we can still check whether model residuals or fit vary across counties (for example by population size or region). If large differences appear, we will report them as limitations and avoid overgeneralizing.\n",
  "\n",
  " - [X] **D.3 Metric selection**: Have we considered the effects of optimizing for our defined metrics and considered additional metrics?\n",
  "\n",
  "We will use standard regression evaluation (for example goodness of fit and error measures) but will prioritize interpretability and stability over optimizing a single metric. We will compare multiple reasonable specifications to ensure conclusions are not driven by one modeling choice.\n",
  "\n",
  " - [X] **D.4 Explainability**: Can we explain in understandable terms a decision the model made in cases where a justification is needed?\n",
  "\n",
  "We will use interpretable models (for example linear regression with coefficients) and explain findings in plain language. We will tie interpretations directly to observed variables and note when the data do not justify a stronger claim.\n",
  "\n",
  " - [X] **D.5 Communicate limitations**: Have we communicated the shortcomings, limitations, and biases of the model to relevant stakeholders in ways that can be generally understood?\n",
  "\n",
  "Our report will include a limitations section emphasizing that associations are not causation, case reporting varies by testing/reporting practices, and mobility metrics may not represent all populations equally. We will explicitly discourage stigmatizing or punitive interpretations.\n",
  "\n",
  "### E. Deployment\n",
  " - [X] **E.1 Monitoring and evaluation**: Do we have a clear plan to monitor the model and its impacts after it is deployed (e.g., performance monitoring, regular audit of sample predictions, human review of high-stakes decisions, reviewing downstream impacts of errors or low-confidence decisions, testing for concept drift)?\n",
  "\n",
  "We are not deploying a model in production. Still, we will sanity-check outputs (for example extreme spikes, missing dates, or inconsistent values) and document cleaning decisions to reduce errors in the final report.\n",
  "\n",
  " - [X] **E.2 Redress**: Have we discussed with our organization a plan for response if users are harmed by the results (e.g., how does the data science team evaluate these cases and update analysis and models to prevent future harm)?\n",
  "\n",
  "Because this is a course research analysis and not a deployed system, formal redress is not applicable. The main risk is misinterpretation, so we mitigate harm through careful framing, avoiding causal claims, and clearly stating limitations and appropriate uses.\n",
  "\n",
  " - [X] **E.3 Roll back**: Is there a way to turn off or roll back the model in production if necessary?\n",
  "\n",
  "Not applicable because we are not deploying a production model. If issues are found, we can update or remove analyses in the repository and clearly note changes.\n",
  "\n",
  " - [X] **E.4 Unintended use**: Have we taken steps to identify and prevent unintended uses and abuse of the model and do we have a plan to monitor these once the model is deployed?\n",
  "\n",
  "Unintended use is possible if results are used to justify surveillance or punitive restrictions targeted at particular counties. We will avoid producing county rankings meant for enforcement decisions, use neutral language, and emphasize that mobility is an imperfect proxy that should not be used for punitive action.\n"
]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
"## Team Expectations\n",
"\n",
"Our team members are Nicholas Campos, Lui Gazem, Cadence Eastep, Ahgean Davis, and Chuck Davies. We have read and agree to follow the COGS108 Team Policies.\n",
"\n",
"- **Clear Communication:** We will communicate primarily through group chat and scheduled meetings. All members are expected to respond within 24 hours unless previously communicated otherwise. Important decisions will be made during meetings to ensure transparency.\n",
"\n",
"- **Accountability & Deadlines:** Each member is responsible for completing their assigned tasks on time. If someone anticipates difficulty meeting a deadline, they will notify the group as early as possible so adjustments can be made.\n",
"\n",
"- **Equal Contribution:** We expect all members to contribute meaningfully to research, coding, writing, and revisions. Work will be divided fairly, but we will support one another if someone needs assistance.\n",
"\n",
"- **Respectful Conflict Resolution:** If disagreements arise, we will address them respectfully and directly. We will focus on the work rather than personal criticism and aim to resolve issues collaboratively.\n",
"\n",
"- **Quality Control:** Before submission, all members will review the project to ensure clarity, correctness, and completeness. No section will be submitted without at least one additional team member reviewing it.\n",
"\n",
"We are committed to maintaining professionalism, consistency, and mutual respect throughout the project.\n"
   
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
"## Project Timeline Proposal\n",
"\n",
"Our team is currently in Week 7 of the quarter (beginning February 15). The final written report and presentation video are due in Week 10. We meet every Sunday at 2 PM to review progress and assign next steps. Our remaining timeline is as follows:\n",
"\n",
"| Meeting Date | Meeting Time | Completed Before Meeting | Discuss at Meeting |\n",
"|--------------|--------------|--------------------------|--------------------|\n",
"| February 16 | 2 PM | Finalize data cleaning; Complete full exploratory data analysis | Confirm final analysis plan; Assign statistical testing and visualization tasks |\n",
"| February 23 | 2 PM | Complete statistical analysis; Generate all visualizations | Interpret results; Identify key findings; Begin drafting Results section |\n",
"| March 2 | 2 PM | Draft Introduction, Methods, and Results sections | Peer review draft; Revise figures; Outline presentation video structure |\n",
"| March 9 | 2 PM | Draft Discussion and Conclusion; Create presentation slides | Final content edits; Record presentation video |\n",
"| Week 10 Deadline | Before submission time | Final formatting and proofreading | Submit final written report and presentation video |\n",
"\n",
"Responsibilities for analysis, writing, visualization, and video production will be divided evenly among team members. Each major section will be reviewed by at least one additional member prior to submission to ensure clarity and accuracy.\n",
"\n",
"We do not anticipate requiring tools beyond those covered in COGS 108 (Python, pandas, visualization libraries, and standard statistical methods). If additional techniques become necessary, we will consult course materials and TA guidance before implementation.\n"
  ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "16b860a9f5fc21240e9d88c0ee13691518c3ce67be252e54a03b9b5b11bd3c7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
